---
title: 埋点和监控
description: 实习中学到的新知识
date: 2023-11-28
random: tues 
tags:
    - react
---

# 前端视角下的数据上报链路
## 一、数据上报的重要性和要求
  数据驱动是一直非常火的一个概念，通过数据驱动，可以用于优化产品体验，决策产品功能，也用于帮助运营增长等等，还可以用于发现质量问题。能够实现数据驱动，首先需要依赖的就是数据上报。所以说 数据上报是非常重要的一项基础能力。
  那么数据上报，就是收集用户的交互行为信息，进行转存、落库、解析，但数据上报也不是就报上去就完事的，还是有很多细节和要求点。
1. 上报需要尽可能准确，这个是第一位的，因为数据一旦缺失，那么数据就立马缺失了他的价值。
2. 上报需要信息收集，上报脚本本身下发和收集了非常多的标记和信息的，例如用户唯一id,设备信息等等。
3. 上报应该尽可能的自动或者简单，埋点本身是一个复杂并且工作量很大的事情，但是作为 一个程序员，需要尽可能的想办法来简化上报的成本。
  为了解决上述问题，整体链路 每个环节都做了一些事情来保障。 先简述一下整体的上报链路：从上报脚本生成数据、收集数据、上报数据 到BFEAgent做简单处理 再到数据网关层进行收录后推进kafka，然后收集分发服务进行数据处理后存储到不同的存储介质中去。
![image.png](https://vip.helloimg.com/i/2024/05/24/6650989d1cd80.png)

## 二、数据生成
  大部分的数据生成 都封装在同一提供的log-report.js(js-sdk)里面，这个js封装了数据的生成，数据的自动捕获，数据的上报等等能力，能够自动上报一些数据，也能够自定义调用上报一些数据
### 1、buvid生成
  当页面加载完成report sdk之后，首先就会进行本地的buvid检查(cookie中的buvid3)，buvid是我们站内通用的用户标识，web这边基本上都用buvid来做uv数据的统计，所以对于buvid是一个强依赖，如果本地没有buvid会请求接口生成一个，如果没有buvid是不能进行上报的。buvid的生成 是有一套规则的，移动端的生成规则(移动端buvid设计)跟web的不太一样，web端没有移动端那么多比较稳定的环境数据(例如设备号等)，所以buvid使用的是8-4-4-4-12的uuid规则生成的一个5段式的随机数再 加上一个时间戳组成 为一个6段式，本质目的就是减少重复率。使用uuid 加上 时间戳生成的格式，碰撞的概率比被陨石击中的170亿分之一还低。
### 2、spm_id检测
  当有了buvid之后，另外一个强依赖就是spm_id，刚才说的buvid是用户的身份标识，那么spm_id就是业务或者页面的唯一标识。一般申明spm_id有两种方式，一种是在页面里面通过meta标签进行声明，例如：.当然也可以在初始化上报脚本的时候申明。spmid的申请现在可以在北极星上通过平台操作就能够申请到。web端的上报event_id是五段式的，例如首页的是333.1007.a.b.c，其中333.1007就是spm_id，属于页面的身份证号了，后面的a.b.c分别是a代表模块，b代表位置，c代表的是行为，举个例子，某个视频卡的曝光的event_id为333.1007.videocard.title.show,点击了这个视频卡的标题，那么event_id就应该是333.1007.videocard.title.click。
### 3、 运行环境数据
  当有了buvid、spmid之后，还会收集一些浏览器运行环境的数据。现在的前端业务上报常用的有三个上报通道，每个通道对应一个唯一的logid，分别是000014 代表pv上报 000016 代表曝光上报 000017代表点击上报。不同的上报通道对应不同的表结构，所以默认收集的数据也有所区别，例如000014的和000017就有一定的相同点和不同点：![111e.png](https://vip.helloimg.com/i/2024/05/24/665098ea57f02.png)
### 4、行为数据  
  当收集到了默认的运行环境数据之后，pv就可以上报了，但是对于曝光和点击，就属于行为埋点数据了。这些曝光和点击的数据，就需要业务方自己根据业务诉求去实现对应的数据收集，例如曝光埋点，根据不同的业务，对于曝光的时机有所不同，例如有的会在图片加载完成后进行曝光上报，有的图片会在露出1px时进行上报，有的会在露出50%时进行上报，然后会再收集一些私有参数。收集完成后，再调用上报脚本提供的上报方法进行统一的上报。
## 三、数据上报
### 3.1 上报时机
  当上报脚本加载完成，必要信息收集完成之后，就会进行数据的上报，这里要特殊说明一下，web的上pv报时机跟移动端的是有区别的，移动端的时机是在用户离开页面的时候进行数据上报，保证能够取到页面离开时间和整体的页面停留时长，web比较特殊，因为不是系统应用级权限，所以很多情况下不一定能够感知到页面的离开，所以web这边是页面进入的时候就进行了pv的上报。值得一提的是，web端的上报脚本的加载时机 跟pv数据的准确性也有很强的关联性，我们早期做过一个实验，通过图片加载一个204的请求来做基准，把数据脚本的加载时机从页面的head中移到body下，整体数据的丢失率会因为不同页面的逻辑复杂度不同，会导致缺失5%到15%的数据。
### 3.2 上报方法
那么进行数据上报的请求发送也是有选择的。一般市面上web数据上报的方式有以下几种：
1. 直接发送请求
跟请求接口一样，通过ajax发送一个get或者post请求。例如使用axios发送就是
axios.post(url,data)
这种方法会有一个问题，就是如果浏览器关闭或者页面卸载掉了的情况下，请求会cancel掉，导致数据的丢失。 我们也可以通过XMLHttpRequest来发送同步请求(axios或者fetch都是通过回调来实现的异步请求)
const xhr = new XMLHttpRequest()
xhr.open('POST',url,false)
xhr.send(data)
但是这样子也有弊端，会阻塞页面关闭或者阻塞页面重载过程和执行过程。
2. 通过图片来发送请求
  通过创建一个img标签，给他的src赋值来实现图片请求的发送，一般我们服务端会返回204 或者返回一个1px的图片。例如: const reportData = (url,data) { let img = document.createElement('img') img.src = url?data }
  这个方式的好处是，一般浏览器都会保证图片的加载，所以可以来保证图片上报的准确定。一般我们校准pv的时候会通过这种方式来作对比，但是不建议作为常规发布，因为dom操作的成本还是很高的，而且也会阻塞页面卸载和下个页面的打开。
3. sendBeacon
  这个方法的诞生就是为了满足统计或者代码诊断的需要，在页面unload之前，向web服务器发送数据，来保证数据不丢失。他是通过http post 发送一个异步请求。sendBeacon是会异步地向服务器发送数据，同时不会延迟页面的卸载也就不会影响下一个页面的打开。所以他会有一下几个优势：数据可靠、异步传输、性能较好。
navigator.sendBeacon(url, data);
所以答案显而易见，我们选用的就是sendBeacon。
4. 上报数据结构
  这里有一个历史遗留的问题，我们的上报脚本是支持不同的logid上报的，那么不同的logid有不同的表结构，我们是通过什么样的数据结构进行上报的呢？是json？还是xml？还是pb?这里会有一个思考，我们希望尽可能的减少数据的传输体积，那么显然json或者xml是非常大的，因为会有很多的声明符号在面，所以之前我们跟数平约定的方式是 通过'|'分隔，通过字段顺序来跟表结构进行映射。举例：
![614081730.png](https://vip.helloimg.com/i/2024/05/24/6650997fb490e.png)
分隔符带来的问题是
需要严格的顺序约束，如果少了一位，那么后面所有的字段都会错位
如果扩展字段，只能一位一位往后加，扩展性不是特别好
可读性很差
异常字符导致分隔错乱或截断
  我们在上报脚本里面做了一定的字段收敛，日常使用的pv点击曝光都做了抹平，但是对于自定义的logid上报就需要自己保证了，不是特别友好。所以后续会整合一个统一的前端通道，使用pb来进行数据格式的传递。
## 四、 数据传输链路
那么当我们上报到接口之后，是不是直接接口进行存储了呢？显然不是。我们每天web端就有百亿的访问，如果直接写入一定是会有性能问题的。我简单介绍一下相关的链路
根据整体链路来看，我们上报请求到的是BFEAgent，这个是个http接口服务，拿到我们的数据之后，它主要处理三个维度的数据：
不同的接口，主要是不同的来源和使用场景
不同的logid，代表数据的通道或者目的
eventID，用于对数据的logid进行remap
  因为前端是获取不到ip的，所以在我们前端上报这边，BFEAgent其实还帮忙获取了 ip、httt_refer、buvid3、mid、ctime、ua、language，这些数据在服务端获取会更加的准确，然后放在meta信息里面后，传递给lancer。BFEAgent 主要负责就是承载大流量的请求，做一些简单的数据收集和处理之后 转发给 lancer。所以不会做非常复杂的逻辑处理，不然一定会导致性能的下降。
  到了lancer之后，lancer也不是直接写入存储层，所以会有一个网关路由层，来将消息进行分发到不同的kafka里面去 进行数据缓冲，一旦进了kafka，丢失数据的可能性就比较低了
![1708352240.png](https://vip.helloimg.com/i/2024/05/24/665099b1b733e.png)
  然后在lancer-collector层进行数据分发，通过特定的管道写入对应的存储介质中去。 那么这么多存储介质每个都有不同的用处。
ES： 指elasticsearch 是基于lucene开发的搜索服务器，我们常说的ELK中的E就是指他，K就是我们用来查询日志的kibana（ops-log）
Hive：应该是我们最常用的，数据查询使用的Hive表
Kafka：数据存储的时候还是会再次写入kafka，这里的数据跟之前的kafka数据是有区别的经过处理的，一般自己消费数据，或者做一些实时的分析 可以使用kafka数据
## 五、数据清洗
  上面说的hive表，是指业务底表，或者叫ods表，这个表是不对外开发申请的，里面存储着原始数据。数仓的同学会根据不同业务的诉求，会行进不同的处理写入不同的业务dwd表中。比如我们的pv数据，一开始是进入ods_web_pv表中，然后再通过调度任务流入各下游表。而位于dwd层的（dwd_flow_ubt_web_event_detail_l_d）汇总了pv、click、impression 以及直播的一些业务通道的数据。方便我们在一个表里就能够实现ctr的查询。
![3333643541.png](https://vip.helloimg.com/i/2024/05/24/665099ff11e8d.png)
  另外除了数据的汇总聚合流向之外，本身清洗数据也会做一些逻辑，例如会过滤基本的带有spider的爬虫数据，一些脏数据等等。
## 六、前端数据使用
  我们的数据确实写入到了我们的hive表里面或者各个存储介质里面   那么我们能有哪些方式能够消费到我们的上报数据呢
平台可视化查询
我们在北极星上面申请的埋点，一些基本的常规数据应该都能够在北极星上面查到。
如果写入的ES的话，那么我们应该能够在kibana上查到我们的数据（ops-log）。我们就能够基于这个日志平台，去配置我们的业务告警。
如果当前业务有做ab实验，那么也可以通过abtest平台进行实验效果的访问。可以通过搭建指标模型进行指标建设。
进阶sql查询
如果写入了hive表，我们除了北极星之外，如果对于数据有一些特殊的使用要求，也可以申请dwd的权限之后，在berserker平台上通过自定义sql进行数据查询。
一般我们的hive表是小时级或者T+1的，那么如果对于数据有实时性的诉求，可以选择自己消费kafka流，来实时的获取数据。消费kafka流可以自己通过搭建服务来完成，也可以使用数平提供的流计算能力，通过flink bsql 写入clickhouse，然后通过clickhouse来进行数据查询。新的Billion2.0能够支持clickhouse的数据查询和告警规则的配置。
![51069952.png](https://vip.helloimg.com/i/2024/05/24/665099e7d1c0c.png)
报表模板化查询
如果要搭建一些报表相关，可以使用berserker的调度任务（job）,将数据根据业务诉求洗入临时的presto或别的中间存储数据表中，然后通过观远来搭建报表。也可以写入mysql等数据源之后通过moni平台来搭建报表。

